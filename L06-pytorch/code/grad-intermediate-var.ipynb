{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"grad-intermediate-var.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"jEbhx3j_p6xw","colab_type":"text"},"source":["STAT 453: Deep Learning (Spring 2020)  \n","Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n","\n","Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2020/  \n","GitHub repository: https://github.com/rasbt/stat453-deep-learning-ss20"]},{"cell_type":"markdown","metadata":{"id":"aw4A09e9p6x1","colab_type":"text"},"source":["# Example Showing How to Get Gradients of an Intermediate Variable in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"DlISGpEUp6x1","colab_type":"text"},"source":["This notebook illustrates how we can fetch the intermediate gradients of a function that is composed of multiple inputs and multiple computation steps in PyTorch. Note that gradient is simply a vector listing the derivatives of a function with respect\n","to each argument of the function. So, strictly speaking, we are discussing how to obtain the partial derivatives here."]},{"cell_type":"markdown","metadata":{"id":"HEQDmaSXp6x2","colab_type":"text"},"source":["Assume we have the simple toy from slides"]},{"cell_type":"markdown","metadata":{"id":"DmoRnC5Lp6x3","colab_type":"text"},"source":["For instance, if we are interested in obtaining the partial derivative of the output a with respect to each of the input and intermediate nodes, we could do the following in PyTorch, where `d_a_b` denotes \"partial derivative of a with respect to b\" and so forth:"]},{"cell_type":"markdown","metadata":{"id":"DaSffCvBp6x3","colab_type":"text"},"source":["## Intermediate Gradients in PyTorch via autograd's `grad`"]},{"cell_type":"markdown","metadata":{"id":"eSBtZd-0p6x4","colab_type":"text"},"source":["In PyTorch, there are multiple ways to compute partial derivatives or gradients. If the goal is to just compute partial derivatives, the most straightforward way would be using `torch.autograd`'s `grad` function. By default, the `retain_graph` parameter of the `grad` function is set to `False`, which will free the graph after computing the partial derivative. Thus, if we want to obtain multiple partial derivatives, we need to set `retain_graph=True`. Note that this is a very inefficient solution though, as multiple passes over the graph are being made where intermediate results are being recalculated:"]},{"cell_type":"markdown","metadata":{"id":"J6df49flp6x6","colab_type":"text"},"source":["As Adam Paszke (PyTorch developer) suggested to me, this can be made in a efficient manner by passing a tuple to the `grad` function so that it can reuse intermediate results and only require one pass over the graph:"]},{"cell_type":"code","metadata":{"id":"-SFPkD0Mp6x7","colab_type":"code","outputId":"e2ade7fd-059a-4800-8bd9-d300669936b3","executionInfo":{"status":"ok","timestamp":1590429836681,"user_tz":180,"elapsed":3301,"user":{"displayName":"Dalcimar Casanova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZzFT0FCo6nTJjXLoCVlWF617XKFK9oco_RLrc-A=s64","userId":"01490701818826847808"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["import torch\n","import torch.nn.functional as F\n","from torch.autograd import grad\n","\n","\n","x = torch.tensor([3.], requires_grad=True)\n","w = torch.tensor([2.], requires_grad=True)\n","b = torch.tensor([1.], requires_grad=True)\n","\n","u = x * w\n","v = u + b\n","a = F.relu(v)\n","\n","partial_derivatives = grad(a, (x, w, b, u, v))\n","\n","for name, grad in zip(\"xwbuv\", (partial_derivatives)):\n","    print('d_a_%s:' % name, grad)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["d_a_x: tensor([2.])\n","d_a_w: tensor([3.])\n","d_a_b: tensor([1.])\n","d_a_u: tensor([1.])\n","d_a_v: tensor([1.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HomJ-l6ip6x9","colab_type":"text"},"source":["## Intermediate Gradients in PyTorch via `retain_grad`"]},{"cell_type":"markdown","metadata":{"id":"YiFYcxK2p6x9","colab_type":"text"},"source":["In PyTorch, we most often use the `backward()` method on an output variable to compute its partial derivative (or gradient) with respect to its inputs (typically, the weights and bias units of a neural network). By default, PyTorch only stores the gradients of the leaf variables (e.g., the weights and biases) via their `grad` attribute to save memory. So, if we are interested in the intermediate results in a computational graph, we can use the `retain_grad` method to store gradients of non-leaf variables as follows:"]},{"cell_type":"code","metadata":{"id":"QTPbVk_Vp6x9","colab_type":"code","outputId":"baf87014-4048-4572-c11b-fa4b94e7f40e","executionInfo":{"status":"ok","timestamp":1590429967494,"user_tz":180,"elapsed":632,"user":{"displayName":"Dalcimar Casanova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZzFT0FCo6nTJjXLoCVlWF617XKFK9oco_RLrc-A=s64","userId":"01490701818826847808"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["import torch\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","x = torch.tensor([3.], requires_grad=True)\n","w = torch.tensor([2.], requires_grad=True)\n","b = torch.tensor([1.], requires_grad=True)\n","\n","u = x * w\n","v = u + b\n","a = F.relu(v)\n","\n","u.retain_grad()\n","v.retain_grad()\n","\n","a.backward()\n","\n","for name, var in zip(\"xwbuv\", (x, w, b, u, v)):\n","    print('d_a_%s:' % name, var.grad)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["d_a_x: tensor([2.])\n","d_a_w: tensor([3.])\n","d_a_b: tensor([1.])\n","d_a_u: tensor([1.])\n","d_a_v: tensor([1.])\n"],"name":"stdout"}]}]}