{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"pytorch-autograd.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Uef1Ydp3ob-x","colab_type":"text"},"source":["STAT 453: Deep Learning (Spring 2020)  \n","Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n","\n","Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2020/  \n","GitHub repository: https://github.com/rasbt/stat453-deep-learning-ss20"]},{"cell_type":"markdown","metadata":{"id":"ySVWa8Fdob-2","colab_type":"text"},"source":["## Autograd Example"]},{"cell_type":"code","metadata":{"id":"E95PyeUIob-2","colab_type":"code","colab":{}},"source":["import torch\n","from torch.autograd import grad\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DvEU9giKob-4","colab_type":"text"},"source":["Suppose we have the example from the lecture slides\n","\n","In PyTorch, the function is defined and computed as follows:"]},{"cell_type":"code","metadata":{"id":"uhf6qGRYob-5","colab_type":"code","colab":{}},"source":["x = torch.tensor([3.])\n","w = torch.tensor([2.], requires_grad=True)\n","b = torch.tensor([1.], requires_grad=True)\n","a = F.relu(x*w + b)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FduClJ4bob-7","colab_type":"code","outputId":"0be430df-d556-4487-b3ab-6691cfd64fb7","executionInfo":{"status":"ok","timestamp":1590429663454,"user_tz":180,"elapsed":1105,"user":{"displayName":"Dalcimar Casanova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZzFT0FCo6nTJjXLoCVlWF617XKFK9oco_RLrc-A=s64","userId":"01490701818826847808"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["a"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([7.], grad_fn=<ReluBackward0>)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"WBaDIIkFob--","colab_type":"text"},"source":["By default, PyTorch will automatically build a computation graph in the background if variables have the parameter `requires_grad=True` set. If new variables without that parameter set to `True` are used in a computation with a variable that has `requires_grad=True`, these new variables will also automatically have gradients set to `True` (this simply means that gradients for these variables will be computed; it is wasteful to set it to `True` if we don't need that variable's gradient; for example, we usually don't need the gradients of he training inputs `x`)."]},{"cell_type":"markdown","metadata":{"id":"C2EB_ibNob--","colab_type":"text"},"source":["Let's compute the derivative of a with respect to w:"]},{"cell_type":"code","metadata":{"id":"750vv55Eob-_","colab_type":"code","outputId":"56b239d7-2363-4cef-b139-3e3f452d142c","executionInfo":{"status":"ok","timestamp":1590429714344,"user_tz":180,"elapsed":830,"user":{"displayName":"Dalcimar Casanova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZzFT0FCo6nTJjXLoCVlWF617XKFK9oco_RLrc-A=s64","userId":"01490701818826847808"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["grad(a, w, retain_graph=True)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([3.]),)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"kcmMuDLyob_B","colab_type":"text"},"source":["Above, the `retain_graph=True` means the computation graph will be kept in memory -- this is for example purposes so that we can use the `grad` function again below. In practice, we usually want to free the computation graph in every round."]},{"cell_type":"code","metadata":{"id":"UKNJCdx3ob_B","colab_type":"code","outputId":"3daf6981-6be2-4a08-9ed7-f6a15a0fbc1c","executionInfo":{"status":"ok","timestamp":1590429734229,"user_tz":180,"elapsed":804,"user":{"displayName":"Dalcimar Casanova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZzFT0FCo6nTJjXLoCVlWF617XKFK9oco_RLrc-A=s64","userId":"01490701818826847808"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["grad(a, b)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1.]),)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"pWaoCv8Wob_E","colab_type":"text"},"source":["Note that PyTorch functions are usually more efficient, but we could also implement our own ReLU function as shown below:\n","\n","Note that even though the derivative of ReLU is not defined at 0, PyTorch autograd will do something that is reasonable for practical purposes:"]},{"cell_type":"code","metadata":{"id":"_u7yXt_2ob_E","colab_type":"code","outputId":"be41b57c-b839-4b32-c7bf-a9f0aeeeac25","executionInfo":{"status":"ok","timestamp":1590429768157,"user_tz":180,"elapsed":903,"user":{"displayName":"Dalcimar Casanova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZzFT0FCo6nTJjXLoCVlWF617XKFK9oco_RLrc-A=s64","userId":"01490701818826847808"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x = torch.tensor([3.])\n","w = torch.tensor([2.], requires_grad=True)\n","b = torch.tensor([1.], requires_grad=True)\n","\n","def my_relu(z):\n","    if z > 0.:\n","        return z\n","    else:\n","        z[:] = 0.\n","        return z\n","\n","a = my_relu(x*w + b)\n","grad(a, w)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([3.]),)"]},"metadata":{"tags":[]},"execution_count":6}]}]}